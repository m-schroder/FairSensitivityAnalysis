{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess fairness and accuracy of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from torchmetrics import AUROC, F1Score, Accuracy, MeanSquaredError\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modules.helpers import seed\n",
    "from modules import evaluation\n",
    "from modules.predictors_modules import StandardClf, FairClf, FairClf_naive, Density_estimator, StandardRegressor, FairRegressor, FairClf_naive_lambda, FairClf_lambda\n",
    "\n",
    "from modules import helpers\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location = \"../models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "# propensity model:\n",
    "label_col = \"A\"\n",
    "\n",
    "experiment_list = [\"sim_cont_d\"]        # Here enter experiments to be evaluated\n",
    "for experiment in experiment_list:\n",
    "\n",
    "    data_location = \"../data/simulator/\"+ experiment + \"_full_dataframe\"   \n",
    "    checkpointpath_density = model_location + \"/density_estimator_A_\" + experiment + \"/density_estimator_A_\" + experiment + \"_checkpoints.ckpt\"\n",
    "    density_estimator = Density_estimator.load_from_checkpoint(checkpointpath_density)\n",
    "\n",
    "    with open(data_location, \"rb\") as input:\n",
    "        full_data = pickle.load(input)\n",
    "\n",
    "    full_data = full_data.drop([\"Y\", \"M\", \"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "    auroc = AUROC(task=\"binary\")\n",
    "\n",
    "    test_data = full_data.iloc[int(0.8*full_data.shape[0]):,:]\n",
    "    covariates = test_data.iloc[:, test_data.columns != label_col].values\n",
    "    covariates = torch.tensor(covariates).float()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits, out = density_estimator(covariates)\n",
    "        auc = auroc(out, torch.tensor(test_data[label_col].to_numpy()).long())\n",
    "        print(\"AUC = \" + str(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "# density estimator:\n",
    "label_col = \"M\"\n",
    "\n",
    "experiment_list = [\"sim_cont_e\"]        # Here enter experiments to be evaluated\n",
    "\n",
    "for experiment in experiment_list:\n",
    "\n",
    "    data_location = \"../data/simulator/\"+ experiment + \"_full_dataframe\"  \n",
    "    checkpointpath_density = model_location + \"/density_estimator_\" + experiment + \"/density_estimator_\" + experiment + \"_checkpoints.ckpt\"\n",
    "    density_estimator = Density_estimator.load_from_checkpoint(checkpointpath_density)\n",
    "\n",
    "    with open(data_location, \"rb\") as input:\n",
    "        full_data = pickle.load(input)\n",
    "\n",
    "    full_data = full_data.drop([\"Y\", \"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "    auroc = AUROC(task=\"binary\")\n",
    "\n",
    "    test_data = full_data.iloc[int(0.8*full_data.shape[0]):,:]\n",
    "    covariates = test_data.iloc[:, test_data.columns != label_col].values\n",
    "    covariates = torch.tensor(covariates).float()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits, out = density_estimator(covariates)\n",
    "        auc = auroc(out, torch.tensor(test_data[label_col].to_numpy()).long())\n",
    "        print(\"AUC = \" + str(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "# StandardClf:\n",
    "label_col = \"Y\"\n",
    "experiment_list = [\"sim_cont_e\"]        # Here enter experiments to be evaluated\n",
    "\n",
    "for experiment in experiment_list:\n",
    "\n",
    "    data_location = \"../data/simulator/\"+ experiment + \"_full_dataframe\"\n",
    "    standard = StandardClf.load_from_checkpoint(model_location + \"StandardClf_\"+ experiment + \"/StandardClf_\"+ experiment + \"_checkpoints.ckpt\")\n",
    "\n",
    "    with open(data_location, \"rb\") as input:\n",
    "        full_data = pickle.load(input)\n",
    "    full_data = full_data.drop([\"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "\n",
    "    auroc = AUROC(task=\"binary\")\n",
    "    test_data = full_data.iloc[int(0.8*full_data.shape[0]):,:]\n",
    "    covariates = test_data.iloc[:, test_data.columns != label_col].values\n",
    "    covariates = torch.tensor(covariates).float()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits, out = standard(covariates)\n",
    "        auc = auroc(out, torch.tensor(test_data[label_col].to_numpy()).long())\n",
    "        print(\"AUC = \" + str(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "# FaiClf_naive:\n",
    "label_col = \"Y\"\n",
    "experiment_list = [\"sim_cont_e\"]        # Here enter experiments to be evaluated\n",
    "\n",
    "for experiment in experiment_list:\n",
    "    naive = FairClf_naive.load_from_checkpoint(model_location + \"FairClf_naive_lambda\"+ experiment + \"/FairClf_naive_lambda\"+ experiment + \"_checkpoints.ckpt\")\n",
    "    data_location = \"../data/simulator/\"+ experiment + \"_full_dataframe\"\n",
    "\n",
    "    with open(data_location, \"rb\") as input:\n",
    "        full_data = pickle.load(input)\n",
    "\n",
    "    full_data = full_data.drop([\"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "    auroc = AUROC(task=\"binary\")\n",
    "\n",
    "    test_data = full_data.iloc[int(0.8*full_data.shape[0]):,:]\n",
    "    covariates = test_data.iloc[:, test_data.columns != label_col].values\n",
    "    covariates = torch.tensor(covariates).float()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits, out = naive(covariates)\n",
    "        auc = auroc(out, torch.tensor(test_data[label_col].to_numpy()).long())\n",
    "        print(\"AUC = \" + str(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "# FaiClf:\n",
    "label_col = \"Y\"\n",
    "experiment_list = [\"sim9\"]        # Here enter experiments to be evaluated\n",
    "\n",
    "for experiment in experiment_list:\n",
    "    fair = FairClf.load_from_checkpoint(model_location + \"FairClf_\"+ experiment + \"/FairClf_\"+ experiment + \"_checkpoints.ckpt\")\n",
    "    data_location = \"../data/simulator/\"+ experiment + \"_full_dataframe\"\n",
    "\n",
    "    with open(data_location, \"rb\") as input:\n",
    "        full_data = pickle.load(input)\n",
    "\n",
    "    full_data = full_data.drop([\"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "    auroc = AUROC(task=\"binary\")\n",
    "\n",
    "    test_data = full_data.iloc[int(0.8*full_data.shape[0]):,:]\n",
    "    covariates = test_data.iloc[:, test_data.columns != label_col].values\n",
    "    covariates = torch.tensor(covariates).float()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits, out = fair(covariates)\n",
    "        auc = auroc(out, torch.tensor(test_data[label_col].to_numpy()).long())\n",
    "        print(\"AUC = \" + str(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Real-world regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "# density:\n",
    "label_col = \"M\"\n",
    "\n",
    "experiment_list = [\"prison\"]\n",
    "for experiment in experiment_list:\n",
    "\n",
    "    data_location = \"../data/prison/prison_dataframe\"    \n",
    "    checkpointpath_density = model_location + \"/density_estimator_\" + experiment + \"/density_estimator_\" + experiment + \"_checkpoints.ckpt\"\n",
    "    density_estimator = Density_estimator.load_from_checkpoint(checkpointpath_density)\n",
    "\n",
    "    with open(data_location, \"rb\") as input:\n",
    "        full_data = pickle.load(input)\n",
    "\n",
    "    full_data = full_data.drop([\"Y\", \"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "    accuracy = Accuracy(task=\"multiclass\", num_classes= len(full_data[\"M\"].unique()), average='weighted')\n",
    "\n",
    "    test_data = full_data.iloc[int(0.8*full_data.shape[0]):,:]\n",
    "    covariates = test_data.iloc[:, test_data.columns != label_col].values\n",
    "    covariates = torch.tensor(covariates).float()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits, out = density_estimator(covariates)\n",
    "        acc = accuracy(out, torch.tensor(test_data[label_col].to_numpy()).long())\n",
    "        print(\"Accuracy = \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "# StandardRegressor:\n",
    "\n",
    "label_col = \"Y\"\n",
    "experiment_list = [\"prison\"]\n",
    "\n",
    "for experiment in experiment_list:\n",
    "\n",
    "    data_location = \"../data/prison/prison_dataframe\"    \n",
    "    standard = StandardRegressor.load_from_checkpoint(model_location + \"StandardRegressor_\"+ experiment + \"/StandardRegressor_\"+ experiment + \"_checkpoints.ckpt\")\n",
    "\n",
    "    with open(data_location, \"rb\") as input:\n",
    "        full_data = pickle.load(input)\n",
    "\n",
    "    full_data = full_data.drop([\"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "    mse = MeanSquaredError()\n",
    "\n",
    "    test_data = full_data.iloc[int(0.8*full_data.shape[0]):,:]\n",
    "    covariates = test_data.iloc[:, test_data.columns != label_col].values\n",
    "    covariates = torch.tensor(covariates).float()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = standard(covariates)\n",
    "        result = mse(out.squeeze(), torch.tensor(test_data[label_col].to_numpy()).long())\n",
    "        print(\"MSE = \" + str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "# FairRegressor:\n",
    "label_col = \"Y\"\n",
    "experiment_list = [\"prison_sensitivity_4\"]\n",
    "\n",
    "for experiment in experiment_list:\n",
    "\n",
    "    data_location = \"../data/prison/prison_dataframe\"    \n",
    "    fair = FairRegressor.load_from_checkpoint(model_location + \"FairRegressor_\"+ experiment + \"/FairRegressor_\"+ experiment + \"_checkpoints.ckpt\")\n",
    "\n",
    "    with open(data_location, \"rb\") as input:\n",
    "        full_data = pickle.load(input)\n",
    "\n",
    "    full_data = full_data.drop([\"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "    mse = MeanSquaredError()\n",
    "\n",
    "    test_data = full_data.iloc[int(0.8*full_data.shape[0]):,:]\n",
    "    covariates = test_data.iloc[:, test_data.columns != label_col].values\n",
    "    covariates = torch.tensor(covariates).float()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = fair(covariates)\n",
    "        result = mse(out.squeeze(), torch.tensor(test_data[label_col].to_numpy()).long())\n",
    "        print(\"MSE = \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate fairness utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment UDE\n",
    "\n",
    "seed(1)\n",
    "\n",
    "label_col = \"Y\"\n",
    "experiment_list = [\"sim9b\", \"sim9c\", \"sim9d\", \"sim9e\"]\n",
    "\n",
    "sensitivity_parameter_list = [2.0]\n",
    "checkpoints = \"_checkpoints.ckpt\"\n",
    "\n",
    "for experiment in experiment_list:\n",
    "    standard = StandardClf.load_from_checkpoint(model_location + \"StandardClf_\"+ experiment + \"/StandardClf_\"+ experiment + \"_checkpoints.ckpt\")\n",
    "    naive = FairClf_naive.load_from_checkpoint(model_location + \"FairClf_naive_\"+ experiment + \"/FairClf_naive_\"+ experiment + \"_checkpoints.ckpt\")\n",
    "    fair = FairClf.load_from_checkpoint(model_location + \"FairClf_\"+ experiment + \"/FairClf_\"+ experiment + \"_checkpoints.ckpt\")\n",
    "    data_location = \"../data/simulator/\"+ experiment + \"_full_dataframe\"\n",
    "\n",
    "    with open(data_location, \"rb\") as input:\n",
    "        full_data = pickle.load(input)\n",
    "\n",
    "    full_data = full_data.drop([\"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "    auroc = AUROC(task=\"binary\")\n",
    "\n",
    "    test_data = full_data.iloc[int(0.8*full_data.shape[0]):,:]\n",
    "    covariates = test_data.iloc[:, test_data.columns != label_col].values\n",
    "    covariates = torch.tensor(covariates).float()\n",
    "\n",
    "    bounds, oracle = evaluation.calculate_bounds(label_col = \"Y\", sensitive_attributes = [0.0, 1.0], sensitivity_parameter_list = sensitivity_parameter_list, checkpoints=checkpoints, experiment_list = experiment_list)\n",
    "    #print(bounds)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, out = standard(covariates)\n",
    "        auc_standard = auroc(out, torch.tensor(test_data[label_col].to_numpy()).long())\n",
    "        print(\"AUC naive = \" + str(auc_standard))\n",
    "\n",
    "        logits, out = naive(covariates)\n",
    "        auc_naive = auroc(out, torch.tensor(test_data[label_col].to_numpy()).long())\n",
    "        print(\"AUC naive = \" + str(auc_naive))\n",
    "\n",
    "        logits, out = fair(covariates)\n",
    "        auc_fair = auroc(out, torch.tensor(test_data[label_col].to_numpy()).long())\n",
    "        print(\"AUC fair = \" + str(auc_fair))\n",
    "\n",
    "    bounds_standard = bounds.loc[bounds[\"Model\"]==\"StandardClf\"]\n",
    "    bounds_naive = bounds.loc[bounds[\"Model\"]==\"FairClf_naive\"]\n",
    "    bounds_fair = bounds.loc[bounds[\"Model\"]==\"FairClf\"]\n",
    "    \n",
    "    utility_standard = 0.5*auc_standard - 0.5*(1/3*np.max(np.abs(bounds_standard[[\"DE_ub\", \"DE_lb\"]])) + 1/3*np.max(np.abs(bounds_standard[[\"IE_ub\", \"IE_lb\"]])) + 1/3*np.max(np.abs(bounds_standard[[\"SE_ub\", \"SE_lb\"]])))\n",
    "    utility_naive = 0.5*auc_naive - 0.5*(1/3*np.max(np.abs(bounds_naive[[\"DE_ub\", \"DE_lb\"]])) + 1/3*np.max(np.abs(bounds_naive[[\"IE_ub\", \"IE_lb\"]])) + 1/3*np.max(np.abs(bounds_naive[[\"SE_ub\", \"SE_lb\"]])))\n",
    "    utility_fair = 0.5*auc_fair - 0.5*(1/3*np.max(np.abs(bounds_fair[[\"DE_ub\", \"DE_lb\"]])) + 1/3*np.max(np.abs(bounds_fair[[\"IE_ub\", \"IE_lb\"]])) + 1/3*np.max(np.abs(bounds_fair[[\"SE_ub\", \"SE_lb\"]])))\n",
    "\n",
    "    print(\"Utility standard = \" + str(utility_standard))\n",
    "    print(\"Utility naive = \" + str(utility_naive))\n",
    "    print(\"Utility fair = \" + str(utility_fair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment UIE\n",
    "\n",
    "seed(1)\n",
    "\n",
    "label_col = \"Y\"\n",
    "experiment_list = [\"sim10b\", \"sim10c\", \"sim10d\", \"sim10e\"]\n",
    "\n",
    "sensitivity_parameter_list = [2.0]\n",
    "checkpoints = \"_checkpoints.ckpt\"\n",
    "\n",
    "for experiment in experiment_list:\n",
    "    standard = StandardClf.load_from_checkpoint(model_location + \"StandardClf_\"+ experiment + \"/StandardClf_\"+ experiment + \"_checkpoints.ckpt\")\n",
    "    naive = FairClf_naive.load_from_checkpoint(model_location + \"FairClf_naive_\"+ experiment + \"/FairClf_naive_\"+ experiment + \"_checkpoints.ckpt\")\n",
    "    fair = FairClf.load_from_checkpoint(model_location + \"FairClf_\"+ experiment + \"/FairClf_\"+ experiment + \"_checkpoints.ckpt\")\n",
    "    data_location = \"../data/simulator/\"+ experiment + \"_full_dataframe\"\n",
    "\n",
    "    with open(data_location, \"rb\") as input:\n",
    "        full_data = pickle.load(input)\n",
    "\n",
    "    full_data = full_data.drop([\"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "    auroc = AUROC(task=\"binary\")\n",
    "\n",
    "    test_data = full_data.iloc[int(0.8*full_data.shape[0]):,:]\n",
    "    covariates = test_data.iloc[:, test_data.columns != label_col].values\n",
    "    covariates = torch.tensor(covariates).float()\n",
    "\n",
    "    bounds, oracle = evaluation.calculate_bounds(label_col = \"Y\", sensitive_attributes = [0.0, 1.0], sensitivity_parameter_list = sensitivity_parameter_list, checkpoints=checkpoints, experiment_list = experiment_list)\n",
    "    #print(bounds)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, out = standard(covariates)\n",
    "        auc_standard = auroc(out, torch.tensor(test_data[label_col].to_numpy()).long())\n",
    "        print(\"AUC naive = \" + str(auc_standard))\n",
    "\n",
    "        logits, out = naive(covariates)\n",
    "        auc_naive = auroc(out, torch.tensor(test_data[label_col].to_numpy()).long())\n",
    "        print(\"AUC naive = \" + str(auc_naive))\n",
    "\n",
    "        logits, out = fair(covariates)\n",
    "        auc_fair = auroc(out, torch.tensor(test_data[label_col].to_numpy()).long())\n",
    "        print(\"AUC fair = \" + str(auc_fair))\n",
    "\n",
    "    bounds_standard = bounds.loc[bounds[\"Model\"]==\"StandardClf\"]\n",
    "    bounds_naive = bounds.loc[bounds[\"Model\"]==\"FairClf_naive\"]\n",
    "    bounds_fair = bounds.loc[bounds[\"Model\"]==\"FairClf\"]\n",
    "    \n",
    "    utility_standard = 0.5*auc_standard - 0.5*(1/3*np.max(np.abs(bounds_standard[[\"DE_ub\", \"DE_lb\"]])) + 1/3*np.max(np.abs(bounds_standard[[\"IE_ub\", \"IE_lb\"]])) + 1/3*np.max(np.abs(bounds_standard[[\"SE_ub\", \"SE_lb\"]])))\n",
    "    utility_naive = 0.5*auc_naive - 0.5*(1/3*np.max(np.abs(bounds_naive[[\"DE_ub\", \"DE_lb\"]])) + 1/3*np.max(np.abs(bounds_naive[[\"IE_ub\", \"IE_lb\"]])) + 1/3*np.max(np.abs(bounds_naive[[\"SE_ub\", \"SE_lb\"]])))\n",
    "    utility_fair = 0.5*auc_fair - 0.5*(1/3*np.max(np.abs(bounds_fair[[\"DE_ub\", \"DE_lb\"]])) + 1/3*np.max(np.abs(bounds_fair[[\"IE_ub\", \"IE_lb\"]])) + 1/3*np.max(np.abs(bounds_fair[[\"SE_ub\", \"SE_lb\"]])))\n",
    "\n",
    "    print(\"Utility standard = \" + str(utility_standard))\n",
    "    print(\"Utility naive = \" + str(utility_naive))\n",
    "    print(\"Utility fair = \" + str(utility_fair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment continuous Z\n",
    "\n",
    "seed(1)\n",
    "\n",
    "label_col = \"Y\"\n",
    "experiment_list = [\"sim_cont_b\", \"sim_cont_c\", \"sim_cont_d\", \"sim_cont_e\"]\n",
    "\n",
    "sensitivity_parameter_list = [2.0]\n",
    "checkpoints = \"_checkpoints-v3.ckpt\"\n",
    "\n",
    "for experiment in experiment_list:\n",
    "    standard = StandardClf.load_from_checkpoint(model_location + \"StandardClf_\"+ experiment + \"/StandardClf_\"+ experiment + \"_checkpoints-v3.ckpt\")\n",
    "    naive = FairClf_naive_lambda.load_from_checkpoint(model_location + \"FairClf_naive_lambda\"+ experiment + \"/FairClf_naive_lambda\"+ experiment + \"_checkpoints-v3.ckpt\")\n",
    "    fair = FairClf_lambda.load_from_checkpoint(model_location + \"FairClf_lambda_\"+ experiment + \"/FairClf_lambda_\"+ experiment + \"_checkpoints-v3.ckpt\")\n",
    "    data_location = \"../data/simulator/\"+ experiment + \"_full_dataframe\"\n",
    "\n",
    "    with open(data_location, \"rb\") as input:\n",
    "        full_data = pickle.load(input)\n",
    "\n",
    "    full_data = full_data.drop([\"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "    auroc = AUROC(task=\"binary\")\n",
    "\n",
    "    test_data = full_data.iloc[int(0.8*full_data.shape[0]):,:]\n",
    "    covariates = test_data.iloc[:, test_data.columns != label_col].values\n",
    "    covariates = torch.tensor(covariates).float()\n",
    "\n",
    "    bounds, oracle = evaluation.calculate_bounds_continuous(label_col = \"Y\", sensitive_attributes = [0.0, 1.0], sensitivity_parameter_list = sensitivity_parameter_list, checkpoints=checkpoints, experiment_list = experiment_list)\n",
    "    #print(bounds)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, out = standard(covariates)\n",
    "        auc_standard = auroc(out, torch.tensor(test_data[label_col].to_numpy()).long())\n",
    "        print(\"AUC naive = \" + str(auc_standard))\n",
    "\n",
    "        logits, out = naive(covariates)\n",
    "        auc_naive = auroc(out, torch.tensor(test_data[label_col].to_numpy()).long())\n",
    "        print(\"AUC naive = \" + str(auc_naive))\n",
    "\n",
    "        logits, out = fair(covariates)\n",
    "        auc_fair = auroc(out, torch.tensor(test_data[label_col].to_numpy()).long())\n",
    "        print(\"AUC fair = \" + str(auc_fair))\n",
    "\n",
    "    bounds_standard = bounds.loc[bounds[\"Model\"]==\"StandardClf\"]\n",
    "    bounds_naive = bounds.loc[bounds[\"Model\"]==\"FairClf_naive\"]\n",
    "    bounds_fair = bounds.loc[bounds[\"Model\"]==\"FairClf\"]\n",
    "    \n",
    "    utility_standard = 0.5*auc_standard - 0.5*(1/3*np.max(np.abs(bounds_standard[[\"DE_ub\", \"DE_lb\"]])) + 1/3*np.max(np.abs(bounds_standard[[\"IE_ub\", \"IE_lb\"]])) + 1/3*np.max(np.abs(bounds_standard[[\"SE_ub\", \"SE_lb\"]])))\n",
    "    utility_naive = 0.5*auc_naive - 0.5*(1/3*np.max(np.abs(bounds_naive[[\"DE_ub\", \"DE_lb\"]])) + 1/3*np.max(np.abs(bounds_naive[[\"IE_ub\", \"IE_lb\"]])) + 1/3*np.max(np.abs(bounds_naive[[\"SE_ub\", \"SE_lb\"]])))\n",
    "    utility_fair = 0.5*auc_fair - 0.5*(1/3*np.max(np.abs(bounds_fair[[\"DE_ub\", \"DE_lb\"]])) + 1/3*np.max(np.abs(bounds_fair[[\"IE_ub\", \"IE_lb\"]])) + 1/3*np.max(np.abs(bounds_fair[[\"SE_ub\", \"SE_lb\"]])))\n",
    "\n",
    "    print(\"Utility standard = \" + str(utility_standard))\n",
    "    print(\"Utility naive = \" + str(utility_naive))\n",
    "    print(\"Utility fair = \" + str(utility_fair))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean + Std of bounds for seed = 0 to seed = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results/bounds_list_over_seeds_sim9\", \"rb\") as input:\n",
    "    bounds_matrix = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = bounds_matrix.loc[(bounds_matrix[\"Model\"] == \"StandardClf\") & (bounds_matrix[\"Experiment\"]==\"sim9e\")].drop([\"Sensitivity parameter\", \"Model\", \"Experiment\"], axis = 1)\n",
    "naive = bounds_matrix.loc[(bounds_matrix[\"Model\"] == \"FairClf_naive\") & (bounds_matrix[\"Experiment\"]==\"sim9e\")].drop([\"Sensitivity parameter\", \"Model\", \"Experiment\"], axis = 1)\n",
    "fair = bounds_matrix.loc[(bounds_matrix[\"Model\"] == \"FairClf\") & (bounds_matrix[\"Experiment\"]==\"sim9e\")].drop([\"Sensitivity parameter\", \"Model\", \"Experiment\"], axis = 1)\n",
    "\n",
    "print(\"standard mean = \" + str(standard.mean()))\n",
    "print(\"standard std = \" + str(standard.std()))\n",
    "\n",
    "print(\"naive mean = \" + str(naive.mean()))\n",
    "print(\"naive std = \" + str(naive.std()))\n",
    "\n",
    "print(\"fair mean = \" + str(fair.mean()))\n",
    "print(\"fair std = \" + str(fair.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_list = [\"sim9b\", \"sim9c\", \"sim9d\", \"sim9e\"]\n",
    "conf_level = np.array([1,2,3,4])\n",
    "\n",
    "checkpoint_list = [\"_checkpoints.ckpt\", \"_checkpoints-v1.ckpt\", \"_checkpoints-v2.ckpt\", \"_checkpoints-v3.ckpt\", \"_checkpoints-v4.ckpt\"]\n",
    "sensitivity_parameter_list = [2.0]\n",
    "\n",
    "\n",
    "bounds_matrix = pd.DataFrame([\"Sensitivity parameter \", \"Model\", \"Experiment\", \"DE_ub\", \"DE_lb\", \"IE_ub\", \"IE_lb\", \"SE_ub\", \"SE_lb\"])\n",
    "\n",
    "for checkpoints in checkpoint_list:\n",
    "    with torch.no_grad():   \n",
    "        bounds, oracle = evaluation.calculate_bounds(label_col = \"Y\", sensitive_attributes = [0.0, 1.0], sensitivity_parameter_list = sensitivity_parameter_list, checkpoints=checkpoints, experiment_list = experiment_list)\n",
    "        bounds_matrix = pd.concat([bounds_matrix, bounds])\n",
    "        print(bounds)\n",
    "\n",
    "bounds_matrix = bounds_matrix.iloc[9:,:].drop(0, axis = 1)\n",
    "\n",
    "with open(\"../results/bounds_list_over_seeds_sim9\", \"wb\") as output_file:\n",
    "    pickle.dump(bounds_matrix, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed sensitivity parameter for calculating bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_location = \"../results/plots/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_list = [\"sim10b\", \"sim10c\", \"sim10d\", \"sim10e\"]      # Here choose experiments to be plotted\n",
    "checkpoints = \"_checkpoints.ckpt\"\n",
    "conf_level = np.array([1,2,3,4])\n",
    "\n",
    "seed(1)\n",
    "\n",
    "sensitivity_parameter_list = [1.2]\n",
    "with torch.no_grad():   \n",
    "    bounds, oracle = evaluation.calculate_bounds(label_col = \"Y\", sensitive_attributes = [0.0, 1.0], sensitivity_parameter_list = sensitivity_parameter_list, checkpoints=checkpoints, experiment_list = experiment_list)\n",
    "helpers.plot_bounds_over_confounding(conf_level, bounds, oracle, path = plot_location + \"bounds_over_confounding/bounds_over_confounding_sim10\" + str(sensitivity_parameter_list) +\".pdf\")\n",
    "\n",
    "sensitivity_parameter_list = [2.0]\n",
    "with torch.no_grad():   \n",
    "    bounds, oracle = evaluation.calculate_bounds(label_col = \"Y\", sensitive_attributes = [0.0, 1.0], sensitivity_parameter_list = sensitivity_parameter_list, checkpoints=checkpoints, experiment_list = experiment_list)\n",
    "helpers.plot_bounds_over_confounding(conf_level, bounds, oracle, path = plot_location + \"bounds_over_confounding/bounds_over_confounding_sim10\" + str(sensitivity_parameter_list) +\".pdf\")\n",
    "\n",
    "sensitivity_parameter_list = [5.0]\n",
    "with torch.no_grad():   \n",
    "    bounds, oracle = evaluation.calculate_bounds(label_col = \"Y\", sensitive_attributes = [0.0, 1.0], sensitivity_parameter_list = sensitivity_parameter_list, checkpoints=checkpoints, experiment_list = experiment_list)\n",
    "helpers.plot_bounds_over_confounding(conf_level, bounds, oracle, path = plot_location + \"bounds_over_confounding/bounds_over_confounding_sim10\" + str(sensitivity_parameter_list) +\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_list = [\"sim_cont_b\",\"sim_cont_c\", \"sim_cont_d\", \"sim_cont_e\"]\n",
    "checkpoints = \"_checkpoints-v3.ckpt\"\n",
    "conf_level = np.array([1,2,3,4])\n",
    "\n",
    "seed(2)\n",
    "\n",
    "sensitivity_parameter_list = [2.0]\n",
    "with torch.no_grad():   \n",
    "    bounds, oracle = evaluation.calculate_bounds_continuous(label_col = \"Y\", sensitive_attributes = [1.0, 0.0], sensitivity_parameter_list = sensitivity_parameter_list, checkpoints=checkpoints, experiment_list = experiment_list)\n",
    "helpers.plot_bounds_over_confounding_continuous(conf_level, bounds, path = plot_location + \"bounds_over_confounding/bounds_over_confounding_sim_cont\" + str(sensitivity_parameter_list) +\".pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset fairness (no classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "conf_level = [1,2,3,4]\n",
    "bounds, oracle, naive, tv = evaluation.caculate_data_fairness(sensitive_attributes = [0.0, 1.0], sensitivity_parameter=np.array([1.5, 5.0, 15.0]), experiment_list = [\"sim9b\",\"sim9c\", \"sim9d\", \"sim9e\"])\n",
    "helpers.plot_bounds_data(conf_level, oracle, naive, bounds, tv, path = plot_location + \"bounds_data_sim9.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "conf_level = [1,2,3,4]\n",
    "bounds, oracle, naive, tv = evaluation.caculate_data_fairness(sensitive_attributes = [0.0, 1.0], sensitivity_parameter=np.array([1.5, 2.0, 5.0]), experiment_list = [\"sim10b\",\"sim10c\", \"sim10d\", \"sim10e\"])\n",
    "helpers.plot_bounds_data(conf_level, oracle, naive, bounds, tv, path = plot_location + \"bounds_data_sim10.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "# test data:\n",
    "data_location = \"../data/prison/prison_dataframe\"    \n",
    "experiment = \"prison\"\n",
    "\n",
    "with open(data_location, \"rb\") as input:\n",
    "    full_data = pickle.load(input)\n",
    "\n",
    "full_data = full_data.drop([\"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "\n",
    "test_data = full_data.iloc[int(0.8*full_data.shape[0]):,:]\n",
    "covariates = test_data.iloc[:, test_data.columns != \"Y\"]\n",
    "covariates_white = covariates.loc[covariates[\"A\"]==1.0].values\n",
    "covariates_white = torch.tensor(covariates_white).float()\n",
    "covariates_nonwhite = covariates.loc[covariates[\"A\"]==0.0].values\n",
    "covariates_nonwhite = torch.tensor(covariates_nonwhite).float()\n",
    "covariates = torch.tensor(covariates.values).float()\n",
    "\n",
    "# regressors\n",
    "\n",
    "standard = StandardRegressor.load_from_checkpoint(model_location + \"StandardRegressor_\"+ experiment + \"/StandardRegressor_\"+ experiment + \"_checkpoints-v1.ckpt\")\n",
    "fair = FairRegressor.load_from_checkpoint(model_location + \"FairRegressor_\"+ experiment + \"/FairRegressor_\"+ experiment + \"_checkpoints.ckpt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_standard= standard(covariates).detach().numpy().squeeze()\n",
    "    out_fair= fair(covariates).detach().numpy().squeeze()\n",
    "\n",
    "out_standard_df = pd.DataFrame({\"Sentence length\": out_standard})\n",
    "out_standard_df[\"Race\"] = test_data[\"A\"].values\n",
    "out_standard_df[\"Race\"] = out_standard_df[\"Race\"].replace(0.0, \"non-white\")\n",
    "out_standard_df[\"Race\"] = out_standard_df[\"Race\"].replace(1.0, \"white\")\n",
    "\n",
    "out_fair_df = pd.DataFrame({\"Sentence length\": out_fair})\n",
    "out_fair_df[\"Race\"] = test_data[\"A\"].values\n",
    "out_fair_df[\"Race\"] = out_fair_df[\"Race\"].replace(0.0, \"non-white\")\n",
    "out_fair_df[\"Race\"] = out_fair_df[\"Race\"].replace(1.0, \"white\")\n",
    "\n",
    "helpers.plot_sentence_distribution(out_standard_df, out_fair_df, path = plot_location + \"prison_distribution.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance change based on sensitivity parameter used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(25)\n",
    "\n",
    "experiment_list = [\"prison_sensitivity_1.0\", \"prison_sensitivity_1.5\", \"prison_sensitivity_2.0\", \"prison_sensitivity_2.5\", \"prison_sensitivity_3.0\", \"prison_sensitivity_3.5\", \"prison_sensitivity_4\"]\n",
    "#checkpoint_list = [\"_checkpoints.ckpt\", \"_checkpoints-v1.ckpt\", \"_checkpoints-v2.ckpt\", \"_checkpoints-v3.ckpt\", \"_checkpoints-v4.ckpt\"]\n",
    "checkpoint_list = [\"_checkpoints-v5.ckpt\", \"_checkpoints-v6.ckpt\", \"_checkpoints-v7.ckpt\", \"_checkpoints-v8.ckpt\", \"_checkpoints-v9.ckpt\"]\n",
    "sensitivity_parameters = [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]\n",
    "\n",
    "performance_df = pd.DataFrame(columns=[\"sensitivity parameter\", \"mean\", \"std\", \"std_minus\", \"std_plus\"])\n",
    "\n",
    "ind = 0\n",
    "for experiment in experiment_list:\n",
    "    performance = []\n",
    "    for checkpoints in checkpoint_list:\n",
    "        with torch.no_grad():   \n",
    "            data_location = \"../data/prison/prison_dataframe\"    \n",
    "            fair = FairRegressor.load_from_checkpoint(model_location + \"FairRegressor_\"+ experiment + \"/FairRegressor_\"+ experiment + checkpoints)\n",
    "\n",
    "            with open(data_location, \"rb\") as input:\n",
    "                full_data = pickle.load(input)\n",
    "\n",
    "            full_data = full_data.drop([\"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "            mse = MeanSquaredError()\n",
    "\n",
    "            test_data = full_data.iloc[int(0.8*full_data.shape[0]):,:]\n",
    "            covariates = test_data.iloc[:, test_data.columns != \"Y\"].values\n",
    "            covariates = torch.tensor(covariates).float()\n",
    "\n",
    "            out = fair(covariates)\n",
    "            result = mse(out.squeeze(), torch.tensor(test_data[label_col].to_numpy()).long())\n",
    "            performance = np.append(performance, result)\n",
    "           \n",
    "    mean = np.mean(performance)\n",
    "    std = np.std(performance)\n",
    "    \n",
    "    performance_df = pd.concat([performance_df, pd.DataFrame({\"sensitivity parameter\": [sensitivity_parameters[ind]], \"mean\": [mean], \"std\": [std], \"std_minus\": [mean-std], \"std_plus\": [mean+std]})])\n",
    "    ind += 1\n",
    "\n",
    "print(performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df[\"std_minus\"] = (performance_df[\"std_minus\"]/performance_df[\"mean\"].to_numpy()[0])*100-100\n",
    "performance_df[\"std_plus\"] = (performance_df[\"std_plus\"]/performance_df[\"mean\"].to_numpy()[0])*100-100\n",
    "performance_df[\"mean\"] = (performance_df[\"mean\"]/performance_df[\"mean\"].to_numpy()[0])*100-100\n",
    "\n",
    "helpers.plot_performance_over_sensitivity_param(performance_df, path = plot_location + \"performance_over_sensitivity_param.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
