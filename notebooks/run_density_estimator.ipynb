{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for training density estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger#, WandbLogger\n",
    "#import wandb\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from modules.helpers import seed#, fit_model\n",
    "from modules.predictors_modules import FairClfDataModule, Density_estimator, StandardRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "#wandb.login()\n",
    "\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_list = [\"sim_cont_b\", \"sim_cont_c\", \"sim_cont_d\", \"sim_cont_e\"]\n",
    "\n",
    "save_location = \"../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = dict(\n",
    "    # network architecture\n",
    "    criterion = nn.BCEWithLogitsLoss,\n",
    "    hidden_dim = 64,\n",
    "    dropout = 0.1,\n",
    "    # training\n",
    "    batch_size = 128,\n",
    "    learning_rate = 0.0001,\n",
    "    max_epochs = 300,                                                                                                      \n",
    "    num_workers = 0,    \n",
    "    # optimization\n",
    "    number_of_trials = 10                                      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"prison\"\n",
    "\n",
    "data_location = \"../data/prison/prison_dataframe\"\n",
    "\n",
    "with open(data_location, \"rb\") as input:\n",
    "    full_data = pickle.load(input)\n",
    "\n",
    "full_data = full_data.drop([\"Y\", \"UIE\", \"UDE\", \"USE\"],axis=1)\n",
    "\n",
    "n_classes = len(np.unique(full_data[\"M\"]))\n",
    "n_features = full_data.drop(\"M\", axis = 1).shape[1]\n",
    "\n",
    "class_weights = compute_class_weight(class_weight ='balanced', classes = np.unique(full_data[\"M\"]), y= full_data[\"M\"].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = FairClfDataModule(   \n",
    "    data_dir = data_location,\n",
    "    label_col = \"M\",\n",
    "    batch_size = train_config['batch_size'],\n",
    "    num_workers = train_config['num_workers'],\n",
    "    mode = \"density\"\n",
    "    )\n",
    "model = Density_estimator(\n",
    "    n_features = n_features,\n",
    "    n_classes = n_classes,\n",
    "    hidden_dim = train_config['hidden_dim'],\n",
    "    dropout = train_config['dropout'],\n",
    "    criterion = train_config['criterion'],\n",
    "    class_weights = class_weights,\n",
    "    learning_rate = train_config['learning_rate']\n",
    ")#.to(\"cpu\", dtype=float)\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.5e-4, 5e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\",[8, 16, 32, 64, 128])\n",
    "    hidden_dim = trial.suggest_categorical(\"hidden_dim\",[32, 64, 128, 256])\n",
    "    dropout = trial.suggest_float(\"dropout\",low=0, high=0.5)\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=5, verbose=False, mode=\"min\", check_on_train_epoch_end=True)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=train_config['max_epochs'],\n",
    "        gpus=0,\n",
    "        logger = True,\n",
    "        callbacks = [early_stop_callback],\n",
    "        deterministic=True,\n",
    "        accelerator=\"cpu\"\n",
    "    )\n",
    "    hyperparameters = dict(learning_rate = learning_rate, batch_size = batch_size, hidden_dim = hidden_dim, dropout = dropout)\n",
    "    trainer.logger.log_hyperparams(hyperparameters)\n",
    "    trainer.fit(model, datamodule = dm)\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner()\n",
    "sampler = optuna.samplers.TPESampler(seed=1)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=pruner, sampler = sampler)\n",
    "study.optimize(objective, n_trials=train_config[\"number_of_trials\"], timeout=100000)                 \n",
    "\n",
    "model_name = \"density_estimator_\" + experiment\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Validation loss: {}\".format(trial.value))\n",
    "print(\"  Parameters: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    \n",
    "# Train model with optimized hyperparameters\n",
    "\n",
    "final_model = Density_estimator(\n",
    "    n_features = n_features,\n",
    "    n_classes = n_classes,\n",
    "    hidden_dim = study.best_params[\"hidden_dim\"],\n",
    "    dropout = study.best_params[\"dropout\"],\n",
    "    criterion = train_config['criterion'],\n",
    "    class_weights = class_weights,\n",
    "    learning_rate = study.best_params[\"learning_rate\"]\n",
    ")\n",
    "\n",
    "dm_final = FairClfDataModule(  \n",
    "    num_workers = train_config['num_workers'],\n",
    "    data_dir = data_location,\n",
    "    label_col = \"M\",\n",
    "    batch_size = study.best_params[\"batch_size\"],\n",
    "    mode = \"density\"\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(save_location, name = model_name)\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath= save_location + \"/\" + model_name,\n",
    "    filename= model_name + \"_checkpoints\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\",\n",
    ")\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=5, verbose=False, mode=\"min\", check_on_train_epoch_end=True)\n",
    "\n",
    "final_trainer = pl.Trainer(\n",
    "max_epochs=train_config['max_epochs'],\n",
    "gpus=0,\n",
    "logger = logger,\n",
    "callbacks = [checkpoint_callback, early_stop_callback],\n",
    "deterministic=True,\n",
    "accelerator=\"cpu\"\n",
    ")\n",
    "\n",
    "final_trainer.fit(final_model, dm_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experiment_list:\n",
    "\n",
    "    data_location = \"../data/simulator/\"+ experiment + \"_full_dataframe\"\n",
    "\n",
    "    with open(data_location, \"rb\") as input:\n",
    "        full_data = pickle.load(input)\n",
    "\n",
    "    full_data = full_data.drop([\"Y\", \"USE\", \"UIE\", \"UDE\", \"M\"],axis=1)\n",
    "\n",
    "    n_classes = len(np.unique(full_data[\"A\"]))\n",
    "    if n_classes == 2:\n",
    "        n_classes = 1\n",
    "    n_features = full_data.drop(\"A\", axis = 1).shape[1]\n",
    "\n",
    "    if n_classes == 1:\n",
    "        class_weights = compute_class_weight(class_weight ='balanced', classes = np.float64(np.arange(n_classes+1)), y= full_data[\"A\"].astype(float))\n",
    "    else: \n",
    "        class_weights = compute_class_weight(class_weight ='balanced', classes = np.float64(np.arange(n_classes)), y= full_data[\"A\"].astype(float))\n",
    "\n",
    "\n",
    "    # Configurate model\n",
    "\n",
    "    dm = FairClfDataModule(\n",
    "        data_dir = data_location,\n",
    "        label_col = \"A\",\n",
    "        batch_size = train_config['batch_size'],\n",
    "        num_workers = train_config['num_workers'],\n",
    "        mode = \"density_A\"\n",
    "        )\n",
    "    model = Density_estimator(\n",
    "        n_features = n_features,\n",
    "        n_classes = n_classes,\n",
    "        class_weights = class_weights,\n",
    "        hidden_dim = train_config['hidden_dim'],\n",
    "        dropout = train_config['dropout'],\n",
    "        criterion = train_config['criterion'],\n",
    "        learning_rate = train_config['learning_rate']\n",
    "    )\n",
    "\n",
    "    # Hyperparameter optimization\n",
    "\n",
    "    def objective(trial: optuna.trial.Trial) -> float:\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 5e-4, 5e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\",[64, 128, 256])\n",
    "        hidden_dim = trial.suggest_categorical(\"hidden_dim\",[64, 128, 256])\n",
    "        dropout = trial.suggest_float(\"dropout\",low=0, high=0.5)\n",
    "        early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=20, verbose=False, mode=\"min\", check_on_train_epoch_end=True)\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=train_config['max_epochs'],\n",
    "            gpus=0,\n",
    "            logger = True,\n",
    "            callbacks = [early_stop_callback],\n",
    "            deterministic=True,\n",
    "            accelerator=\"cpu\"\n",
    "        )\n",
    "        hyperparameters = dict(learning_rate = learning_rate, batch_size = batch_size, hidden_dim = hidden_dim, dropout = dropout)\n",
    "        trainer.logger.log_hyperparams(hyperparameters)\n",
    "        trainer.fit(model, datamodule = dm)\n",
    "        return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "    pruner = optuna.pruners.MedianPruner()\n",
    "    sampler = optuna.samplers.TPESampler(seed=1)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=pruner, sampler = sampler)\n",
    "    study.optimize(objective, n_trials=train_config[\"number_of_trials\"], timeout=100000)                 \n",
    "\n",
    "    model_name = \"density_estimator_A_\" + experiment\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Validation loss: {}\".format(trial.value))\n",
    "    print(\"  Parameters: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    \n",
    "# Train model with optimized hyperparameters\n",
    "\n",
    "    final_model = Density_estimator(\n",
    "        n_features = n_features,\n",
    "        n_classes = n_classes,\n",
    "        class_weights = class_weights,\n",
    "        hidden_dim = study.best_params[\"hidden_dim\"],\n",
    "        dropout = study.best_params[\"dropout\"],\n",
    "        criterion = train_config['criterion'],\n",
    "        learning_rate = study.best_params[\"learning_rate\"]\n",
    "    )#.to(\"cpu\", dtype=float)\n",
    "\n",
    "    dm_final = FairClfDataModule(  \n",
    "        num_workers = train_config['num_workers'],\n",
    "        data_dir = data_location,\n",
    "        label_col = \"A\",\n",
    "        batch_size = study.best_params[\"batch_size\"],\n",
    "        mode = \"density_A\"\n",
    "    )\n",
    "\n",
    "\n",
    "    logger = TensorBoardLogger(save_location, name = model_name)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath= save_location + \"/\" + model_name,\n",
    "        filename= model_name + \"_checkpoints\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=10, verbose=False, mode=\"min\", check_on_train_epoch_end=True)\n",
    "\n",
    "    final_trainer = pl.Trainer(\n",
    "    max_epochs=train_config['max_epochs'],\n",
    "    gpus=0,\n",
    "    logger = logger,\n",
    "    callbacks = [checkpoint_callback, early_stop_callback],\n",
    "    deterministic=True,\n",
    "    accelerator=\"cpu\"\n",
    "    )\n",
    "\n",
    "    final_trainer.fit(final_model, dm_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experiment_list:\n",
    "\n",
    "    data_location = \"../data/simulator/\"+ experiment + \"_full_dataframe\"\n",
    "\n",
    "    with open(data_location, \"rb\") as input:\n",
    "        full_data = pickle.load(input)\n",
    "\n",
    "    full_data = full_data.drop([\"Y\", \"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "\n",
    "    n_classes = len(np.unique(full_data[\"M\"]))\n",
    "    if n_classes == 2:\n",
    "        n_classes = 1\n",
    "    n_features = full_data.drop(\"M\", axis = 1).shape[1]\n",
    "\n",
    "    if n_classes == 1:\n",
    "        class_weights = compute_class_weight(class_weight ='balanced', classes = np.float64(np.arange(n_classes+1)), y= full_data[\"M\"].astype(float))\n",
    "    else: \n",
    "        class_weights = compute_class_weight(class_weight ='balanced', classes = np.float64(np.arange(n_classes)), y= full_data[\"M\"].astype(float))\n",
    "\n",
    "\n",
    "    # Configurate model\n",
    "\n",
    "    dm = FairClfDataModule(\n",
    "        data_dir = data_location,\n",
    "        label_col = \"M\",\n",
    "        batch_size = train_config['batch_size'],\n",
    "        num_workers = train_config['num_workers'],\n",
    "        mode = \"density_M\"\n",
    "        )\n",
    "    model = Density_estimator(\n",
    "        n_features = n_features,\n",
    "        n_classes = n_classes,\n",
    "        hidden_dim = train_config['hidden_dim'],\n",
    "        dropout = train_config['dropout'],\n",
    "        criterion = train_config['criterion'],\n",
    "        class_weights = class_weights,\n",
    "        learning_rate = train_config['learning_rate']\n",
    "    )\n",
    "\n",
    "    # Hyperparameter optimization\n",
    "\n",
    "    def objective(trial: optuna.trial.Trial) -> float:\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 5e-4, 5e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\",[64, 128, 256])\n",
    "        hidden_dim = trial.suggest_categorical(\"hidden_dim\",[64, 128, 256])\n",
    "        dropout = trial.suggest_float(\"dropout\",low=0, high=0.5)\n",
    "        early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=20, verbose=False, mode=\"min\", check_on_train_epoch_end=True)\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=train_config['max_epochs'],\n",
    "            gpus=0,\n",
    "            logger = True,\n",
    "            callbacks = [early_stop_callback],\n",
    "            deterministic=True,\n",
    "            accelerator=\"cpu\"\n",
    "        )\n",
    "        hyperparameters = dict(learning_rate = learning_rate, batch_size = batch_size, hidden_dim = hidden_dim, dropout = dropout)\n",
    "        trainer.logger.log_hyperparams(hyperparameters)\n",
    "        trainer.fit(model, datamodule = dm)\n",
    "        return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "    pruner = optuna.pruners.MedianPruner()\n",
    "    sampler = optuna.samplers.TPESampler(seed=1)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=pruner, sampler = sampler)\n",
    "    study.optimize(objective, n_trials=train_config[\"number_of_trials\"], timeout=100000)                 \n",
    "\n",
    "    model_name = \"density_estimator_\" + experiment\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Validation loss: {}\".format(trial.value))\n",
    "    print(\"  Parameters: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    \n",
    "# Train model with optimized hyperparameters\n",
    "\n",
    "    final_model = Density_estimator(\n",
    "        n_features = n_features,\n",
    "        n_classes = n_classes,\n",
    "        hidden_dim = study.best_params[\"hidden_dim\"],\n",
    "        dropout = study.best_params[\"dropout\"],\n",
    "        criterion = train_config['criterion'],\n",
    "        class_weights = class_weights,\n",
    "        learning_rate = study.best_params[\"learning_rate\"]\n",
    "    )#.to(\"cpu\", dtype=float)\n",
    "\n",
    "    dm_final = FairClfDataModule(  \n",
    "        num_workers = train_config['num_workers'],\n",
    "        data_dir = data_location,\n",
    "        label_col = \"M\",\n",
    "        batch_size = study.best_params[\"batch_size\"],\n",
    "        mode = \"density_M\"\n",
    "    )\n",
    "\n",
    "\n",
    "    logger = TensorBoardLogger(save_location, name = model_name)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath= save_location + \"/\" + model_name,\n",
    "        filename= model_name + \"_checkpoints\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=10, verbose=False, mode=\"min\", check_on_train_epoch_end=True)\n",
    "\n",
    "    final_trainer = pl.Trainer(\n",
    "    max_epochs=train_config['max_epochs'],\n",
    "    gpus=0,\n",
    "    logger = logger,\n",
    "    callbacks = [checkpoint_callback, early_stop_callback],\n",
    "    deterministic=True,\n",
    "    accelerator=\"cpu\"\n",
    "    )\n",
    "\n",
    "    final_trainer.fit(final_model, dm_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
