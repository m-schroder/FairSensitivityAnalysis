{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for running the three classifiers (standard, fair_biased, fair) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from modules.helpers import seed\n",
    "from modules.predictors_modules import FairClfDataModule, StandardClf, FairClf, FairClf_naive, StandardRegressor, FairRegressor\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"sim9e\"\n",
    "\n",
    "data_location = \"../data/simulator/\"+ experiment + \"_full_dataframe\"\n",
    "save_location = \"../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_location, \"rb\") as input:\n",
    "    full_data = pickle.load(input)\n",
    "\n",
    "full_data = full_data.drop([\"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "\n",
    "if len(np.unique(full_data[\"Y\"])) > 2:\n",
    "    n_classes = len(np.unique(full_data[\"Y\"]))\n",
    "    class_weights = compute_class_weight(class_weight ='balanced', classes = np.float64(np.arange(n_classes)), y= full_data[\"Y\"].astype(float))\n",
    "else:\n",
    "    n_classes = 1\n",
    "    class_weights = compute_class_weight(class_weight ='balanced', classes = np.float64(np.arange(n_classes+1)), y= full_data[\"Y\"].astype(float))\n",
    "\n",
    "n_features = full_data.drop(\"Y\", axis = 1).shape[1]\n",
    "\n",
    "sensitive_attributes = full_data[\"A\"].unique()\n",
    "node_names = full_data.drop([\"Y\"],axis=1).columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = full_data.loc[:int(0.6*full_data.shape[0]),:].drop(\"Y\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = dict(\n",
    "    # network architecture\n",
    "    criterion = nn.MSELoss,\n",
    "    #criterion = nn.CrossEntropyLoss,\n",
    "    hidden_dim = 64,\n",
    "    dropout = 0.1,\n",
    "    # fairness parameters\n",
    "    sensitivity_parameter = 3.0,\n",
    "    fairness_constraints = [0.2, 0.2, 0.2],   # gamma for restricting DE, IE, SE\n",
    "    # training\n",
    "    batch_size = 128,\n",
    "    learning_rate = 0.0001,\n",
    "    max_epochs = 200,                                                                                                      \n",
    "    num_workers = 0,\n",
    "    nested_epochs_lagrangian = 5,    \n",
    "    # optimization\n",
    "    number_of_trials = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Standard Classifier (no fairness constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_list = [\"sim100d\"]\n",
    "\n",
    "for experiment in experiment_list:\n",
    "\n",
    "    data_location = \"../data/simulator/\"+ experiment + \"_full_dataframe\"\n",
    "    #data_location = \"../data/prison/prison_dataframe\"\n",
    "    save_location = \"../models\"\n",
    "\n",
    "    with open(data_location, \"rb\") as input:\n",
    "        full_data = pickle.load(input)\n",
    "    full_data = full_data.drop([\"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "    if len(np.unique(full_data[\"Y\"])) > 2:\n",
    "        n_classes = len(np.unique(full_data[\"Y\"]))\n",
    "        class_weights = compute_class_weight(class_weight ='balanced', classes = np.float64(np.arange(n_classes)), y= full_data[\"Y\"].astype(float))\n",
    "        #class_weights = None\n",
    "    else:\n",
    "        n_classes = 1\n",
    "        class_weights = compute_class_weight(class_weight ='balanced', classes = np.float64(np.arange(n_classes+1)), y= full_data[\"Y\"].astype(float))\n",
    "    n_features = full_data.drop(\"Y\", axis = 1).shape[1]\n",
    "    sensitive_attributes = full_data[\"A\"].unique()\n",
    "    node_names = full_data.drop([\"Y\"],axis=1).columns.values\n",
    "\n",
    "    train_data = full_data.loc[:int(0.6*full_data.shape[0]),:].drop(\"Y\", axis = 1)\n",
    "\n",
    "    dm = FairClfDataModule(\n",
    "        data_dir = data_location,\n",
    "        label_col = \"Y\",\n",
    "        batch_size = train_config['batch_size'],\n",
    "        num_workers = train_config['num_workers'],\n",
    "        mode = \"prediction\"\n",
    "    )\n",
    "    model = StandardClf(\n",
    "        n_features = n_features,\n",
    "        n_classes = n_classes,\n",
    "        hidden_dim = train_config['hidden_dim'],\n",
    "        dropout = train_config['dropout'],\n",
    "        criterion = train_config['criterion'],\n",
    "        class_weights = class_weights,\n",
    "        learning_rate = train_config['learning_rate']\n",
    "    )#.to(\"cpu\", dtype=float)\n",
    "\n",
    "#Hyperparameter optimization\n",
    "\n",
    "    def objective(trial: optuna.trial.Trial) -> float:\n",
    "       learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 1e-1)\n",
    "       batch_size = trial.suggest_categorical(\"batch_size\",[64, 128, 256])\n",
    "       hidden_dim = trial.suggest_int(\"hidden_dim\",low=16, high=256)\n",
    "       dropout = trial.suggest_float(\"dropout\",low=0, high=0.5)\n",
    "       trainer = pl.Trainer(\n",
    "           max_epochs=train_config['max_epochs'],\n",
    "           gpus=0,\n",
    "           logger = True,\n",
    "           deterministic=True,\n",
    "           accelerator=\"cpu\"\n",
    "       )\n",
    "       hyperparameters = dict(learning_rate = learning_rate, batch_size = batch_size, hidden_dim = hidden_dim, dropout = dropout)\n",
    "       trainer.logger.log_hyperparams(hyperparameters)\n",
    "       trainer.fit(model, datamodule = dm)\n",
    "       return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "    pruner = optuna.pruners.MedianPruner()\n",
    "    sampler = optuna.samplers.TPESampler(seed=1)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=pruner, sampler = sampler)\n",
    "    study.optimize(objective, n_trials=train_config[\"number_of_trials\"], show_progress_bar=False)                \n",
    "\n",
    "    model_name = \"StandardClf_\" + experiment\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Validation loss: {}\".format(trial.value))\n",
    "    print(\"  Parameters: \")\n",
    "    for key, value in trial.params.items():\n",
    "       print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    \n",
    "# Train model with optimized hyperparameters\n",
    "\n",
    "    final_model = StandardClf(\n",
    "       n_features = n_features,\n",
    "       n_classes = n_classes,\n",
    "       hidden_dim = study.best_params[\"hidden_dim\"],\n",
    "       dropout = study.best_params[\"dropout\"],\n",
    "       criterion = train_config['criterion'],\n",
    "       class_weights = class_weights,\n",
    "       learning_rate = study.best_params[\"learning_rate\"]\n",
    "    )\n",
    "\n",
    "    dm_final = FairClfDataModule(  \n",
    "       num_workers = train_config['num_workers'],\n",
    "       data_dir = data_location,\n",
    "       label_col = \"Y\",\n",
    "       batch_size = study.best_params[\"batch_size\"],\n",
    "       mode = \"prediction\"\n",
    "    )\n",
    "\n",
    "    logger = TensorBoardLogger(save_location, name = model_name)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath= save_location + \"/\" + model_name,\n",
    "        filename= model_name + \"_checkpoints\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=10, verbose=False, mode=\"min\", check_on_train_epoch_end=True)\n",
    "\n",
    "    final_trainer = pl.Trainer(\n",
    "    max_epochs=train_config['max_epochs'],\n",
    "    logger = logger,\n",
    "    callbacks = [checkpoint_callback, early_stop_callback],\n",
    "    deterministic=True,\n",
    "    accelerator=\"cpu\",\n",
    "    log_every_n_steps=20\n",
    "    )\n",
    "\n",
    "    final_trainer.fit(final_model, dm_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Naive Fair Classifier (does not account for unobserved confounding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_naive = FairClfDataModule(\n",
    "    data_dir = data_location,\n",
    "    label_col = \"Y\",\n",
    "    batch_size = train_config['batch_size'],\n",
    "    num_workers = train_config['num_workers'],\n",
    "    mode = \"prediction\"\n",
    ")\n",
    "\n",
    "model_naive = FairClf_naive(\n",
    "    n_features = n_features,\n",
    "    n_classes = n_classes,\n",
    "    train_data = train_data,\n",
    "    hidden_dim = train_config['hidden_dim'],\n",
    "    dropout = train_config['dropout'],\n",
    "    batch_size=train_config['batch_size'],\n",
    "    criterion_pred = train_config['criterion'],\n",
    "    class_weights = class_weights,\n",
    "    learning_rate = train_config['learning_rate'],\n",
    "    sensitive_attributes = sensitive_attributes,\n",
    "    constraints = train_config['fairness_constraints'],\n",
    "    nested_epochs = train_config['nested_epochs_lagrangian'],\n",
    "    column_names = node_names\n",
    ").to(\"cpu\", dtype=float)\n",
    "\n",
    "# Hyperparameter optimization\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\",[64, 128, 256])\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\",low=128, high=256)\n",
    "    dropout = trial.suggest_float(\"dropout\",low=0, high=0.5)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=train_config['max_epochs'],\n",
    "        logger = True,\n",
    "        deterministic=True,\n",
    "        accelerator=\"cpu\"\n",
    "    )\n",
    "    hyperparameters = dict(learning_rate = learning_rate, batch_size = batch_size, hidden_dim = hidden_dim, dropout = dropout)\n",
    "    trainer.logger.log_hyperparams(hyperparameters)\n",
    "    trainer.fit(model_naive, datamodule = dm_naive)\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner()\n",
    "sampler = optuna.samplers.TPESampler(seed=1)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=pruner, sampler = sampler)\n",
    "study.optimize(objective, n_trials=train_config[\"number_of_trials\"], timeout=60000, show_progress_bar=False)                 \n",
    "\n",
    "model_name = \"FairClf_naive_\" + experiment\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Validation loss: {}\".format(trial.value))\n",
    "print(\"  Parameters: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    \n",
    "# Train model with optimized hyperparameters\n",
    "\n",
    "final_model_naive = FairClf_naive(\n",
    "    n_features = n_features,\n",
    "    n_classes = n_classes,\n",
    "    train_data = train_data,\n",
    "    hidden_dim = study.best_params[\"hidden_dim\"],\n",
    "    dropout = study.best_params[\"dropout\"],\n",
    "    batch_size=study.best_params[\"batch_size\"],\n",
    "    criterion_pred = train_config['criterion'],\n",
    "    class_weights = class_weights,\n",
    "    learning_rate = study.best_params[\"learning_rate\"],\n",
    "    sensitive_attributes = sensitive_attributes,\n",
    "    constraints = train_config['fairness_constraints'],\n",
    "    nested_epochs = train_config['nested_epochs_lagrangian'],\n",
    "    column_names = node_names\n",
    ").to(\"cpu\", dtype=float)\n",
    "\n",
    "dm_final_naive = FairClfDataModule(  \n",
    "    num_workers = train_config['num_workers'],\n",
    "    data_dir = data_location,\n",
    "    label_col = \"Y\",\n",
    "    batch_size = study.best_params[\"batch_size\"],\n",
    "    mode = \"prediction\"\n",
    ")\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(save_location, name = model_name)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath= save_location + \"/\" + model_name,\n",
    "    filename= model_name + \"_checkpoints\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "final_trainer_naive = pl.Trainer(\n",
    "max_epochs=train_config['max_epochs'],\n",
    "logger = logger,\n",
    "callbacks = [checkpoint_callback],\n",
    "deterministic=True,\n",
    "accelerator=\"cpu\"\n",
    ")\n",
    "\n",
    "final_trainer_naive.fit(final_model_naive, dm_final_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"FairClf_naive_\" + experiment\n",
    "\n",
    "final_model_naive = FairClf_naive(\n",
    "    n_features = n_features,\n",
    "    n_classes = n_classes,\n",
    "    train_data = train_data,\n",
    "    hidden_dim = train_config[\"hidden_dim\"],\n",
    "    dropout = train_config[\"dropout\"],\n",
    "    batch_size=train_config[\"batch_size\"],\n",
    "    criterion_pred = train_config['criterion'],\n",
    "    class_weights = class_weights,\n",
    "    learning_rate = train_config[\"learning_rate\"],\n",
    "    sensitive_attributes = sensitive_attributes,\n",
    "    constraints = train_config['fairness_constraints'],\n",
    "    nested_epochs = train_config['nested_epochs_lagrangian'],\n",
    "    column_names = node_names\n",
    ").to(\"cpu\", dtype=float)\n",
    "\n",
    "dm_final_naive = FairClfDataModule(  \n",
    "    num_workers = train_config['num_workers'],\n",
    "    data_dir = data_location,\n",
    "    label_col = \"Y\",\n",
    "    batch_size = train_config[\"batch_size\"],\n",
    "    mode = \"prediction\"\n",
    ")\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(save_location, name = model_name)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath= save_location + \"/\" + model_name,\n",
    "    filename= model_name + \"_checkpoints\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "final_trainer_naive = pl.Trainer(\n",
    "max_epochs=train_config['max_epochs'],\n",
    "logger = logger,\n",
    "callbacks = [checkpoint_callback],\n",
    "deterministic=True,\n",
    "accelerator=\"cpu\"\n",
    ")\n",
    "\n",
    "final_trainer_naive.fit(final_model_naive, dm_final_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Fair Classifier with bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_list = [\"sim100\"]\n",
    "\n",
    "for experiment in experiment_list:\n",
    "\n",
    "    data_location = \"../data/simulator/\"+ experiment + \"_full_dataframe\"\n",
    "    save_location = \"../models\"\n",
    "\n",
    "    with open(data_location, \"rb\") as input:\n",
    "        full_data = pickle.load(input)\n",
    "    full_data = full_data.drop([\"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "    if len(np.unique(full_data[\"Y\"])) > 2:\n",
    "        n_classes = len(np.unique(full_data[\"Y\"]))\n",
    "        class_weights = compute_class_weight(class_weight ='balanced', classes = np.float64(np.arange(n_classes)), y= full_data[\"Y\"].astype(float))\n",
    "    else:\n",
    "        n_classes = 1\n",
    "        class_weights = compute_class_weight(class_weight ='balanced', classes = np.float64(np.arange(n_classes+1)), y= full_data[\"Y\"].astype(float))\n",
    "    n_features = full_data.drop(\"Y\", axis = 1).shape[1]\n",
    "    sensitive_attributes = full_data[\"A\"].unique()\n",
    "    node_names = full_data.drop([\"Y\"],axis=1).columns.values\n",
    "\n",
    "    train_data = full_data.loc[:int(0.6*full_data.shape[0]),:]\n",
    "\n",
    "    checkpointpath_density = save_location + \"/density_estimator_\" + experiment + \"/density_estimator_\" + experiment + \"_checkpoints.ckpt\"\n",
    "\n",
    "    model_name = \"FairClf_\" + experiment\n",
    "    \n",
    "    final_model = FairClf(\n",
    "        n_features = n_features,\n",
    "        n_classes = n_classes,\n",
    "        train_data = train_data,\n",
    "        hidden_dim = train_config[\"hidden_dim\"],\n",
    "        dropout = train_config[\"dropout\"],\n",
    "        batch_size=train_config[\"batch_size\"],\n",
    "        criterion_pred = train_config['criterion'],\n",
    "        class_weights = class_weights,\n",
    "        learning_rate = train_config[\"learning_rate\"],\n",
    "        sensitive_attributes = sensitive_attributes,\n",
    "        sensitivity_parameter= train_config[\"sensitivity_parameter\"],\n",
    "        checkpointpath_density=checkpointpath_density,\n",
    "        constraints = train_config['fairness_constraints'],\n",
    "        nested_epochs = train_config['nested_epochs_lagrangian'],\n",
    "        column_names = node_names\n",
    "    )\n",
    "    dm_final = FairClfDataModule(  \n",
    "        num_workers = train_config['num_workers'],\n",
    "        data_dir = data_location,\n",
    "        label_col = \"Y\",\n",
    "        batch_size = train_config[\"batch_size\"],\n",
    "        mode = \"prediction\"\n",
    "    )\n",
    "    logger = TensorBoardLogger(save_location, name = model_name)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath= save_location + \"/\" + model_name,\n",
    "        filename= model_name + \"_checkpoints\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_fairness\", min_delta=0.00, patience=5, verbose=False, mode=\"min\", check_on_train_epoch_end=True)\n",
    "\n",
    "    final_trainer = pl.Trainer(\n",
    "        max_epochs=train_config['max_epochs'],\n",
    "        logger = logger,\n",
    "        callbacks = [checkpoint_callback, early_stop_callback],\n",
    "        deterministic=True,\n",
    "        accelerator=\"cpu\"\n",
    "    )\n",
    "\n",
    "    final_trainer.fit(final_model, dm_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-world regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Standard Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"prison\"\n",
    "\n",
    "data_location = \"../data/prison/prison_dataframe\"\n",
    "save_location = \"../models\"\n",
    "\n",
    "with open(data_location, \"rb\") as input:\n",
    "    full_data = pickle.load(input)\n",
    "full_data = full_data.drop([\"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "\n",
    "n_features = full_data.drop(\"Y\", axis = 1).shape[1]\n",
    "sensitive_attributes = full_data[\"A\"].unique()\n",
    "node_names = full_data.drop([\"Y\"],axis=1).columns.values\n",
    "\n",
    "dm = FairClfDataModule(\n",
    "    data_dir = data_location,\n",
    "    label_col = \"Y\",\n",
    "    batch_size = train_config['batch_size'],\n",
    "    num_workers = train_config['num_workers'],\n",
    "     mode = \"prediction\"\n",
    ")\n",
    "model = StandardRegressor(\n",
    "    n_features = n_features,\n",
    "    n_classes = 1,\n",
    "    hidden_dim = train_config['hidden_dim'],\n",
    "    dropout = train_config['dropout'],\n",
    "    criterion = nn.MSELoss,\n",
    "    learning_rate = train_config['learning_rate']\n",
    ")\n",
    "\n",
    "#Hyperparameter optimization\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.5e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\",[8, 16, 32, 64, 128])\n",
    "    hidden_dim = trial.suggest_categorical(\"hidden_dim\",[32, 64, 128, 256])\n",
    "    dropout = trial.suggest_float(\"dropout\",low=0, high=0.5)\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=20, verbose=False, mode=\"min\", check_on_train_epoch_end=True)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=train_config['max_epochs'],\n",
    "        gpus=0,\n",
    "        logger = True,\n",
    "        callbacks=early_stop_callback,\n",
    "        deterministic=True,\n",
    "        accelerator=\"cpu\"\n",
    "    )\n",
    "    hyperparameters = dict(learning_rate = learning_rate, batch_size = batch_size, hidden_dim = hidden_dim, dropout = dropout)\n",
    "    trainer.logger.log_hyperparams(hyperparameters)\n",
    "    trainer.fit(model, datamodule = dm)\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner()\n",
    "sampler = optuna.samplers.TPESampler(seed=1)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=pruner, sampler = sampler)\n",
    "study.optimize(objective, n_trials=train_config[\"number_of_trials\"], show_progress_bar=False)        \n",
    "\n",
    "model_name = \"StandardRegressor_\" + experiment\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Validation loss: {}\".format(trial.value))\n",
    "print(\"  Parameters: \")\n",
    "for key, value in trial.params.items():\n",
    "   print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    \n",
    "# Train model with optimized hyperparameters\n",
    "\n",
    "final_model = StandardRegressor(\n",
    "    n_features = n_features,\n",
    "    n_classes = n_classes,\n",
    "    hidden_dim = study.best_params[\"hidden_dim\"],\n",
    "    dropout = study.best_params[\"dropout\"],\n",
    "    criterion = train_config['criterion'],\n",
    "    learning_rate = study.best_params[\"learning_rate\"]\n",
    ")\n",
    "dm_final = FairClfDataModule(  \n",
    "    num_workers = train_config['num_workers'],\n",
    "    data_dir = data_location,\n",
    "    label_col = \"Y\",\n",
    "    batch_size = study.best_params[\"batch_size\"],\n",
    "    mode = \"prediction\"\n",
    ")\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(save_location, name = model_name)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath= save_location + \"/\" + model_name,\n",
    "    filename= model_name + \"_checkpoints\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=20, verbose=False, mode=\"min\", check_on_train_epoch_end=True)\n",
    "\n",
    "final_trainer = pl.Trainer(\n",
    "max_epochs=train_config['max_epochs'],\n",
    "logger = logger,\n",
    "callbacks = [checkpoint_callback, early_stop_callback],\n",
    "deterministic=True,\n",
    "accelerator=\"cpu\",\n",
    "log_every_n_steps=20\n",
    ")\n",
    "\n",
    "final_trainer.fit(final_model, dm_final)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "max_epochs=train_config['max_epochs'],\n",
    "gpus=0,\n",
    "logger = logger,\n",
    "callbacks = [checkpoint_callback, early_stop_callback],\n",
    "deterministic=True,\n",
    "accelerator=\"cpu\"\n",
    ")\n",
    "\n",
    "final_trainer.fit(final_model, dm_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Fair Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"prison\"\n",
    "model_location = \"../models/\"\n",
    "standard = StandardRegressor.load_from_checkpoint(model_location + \"StandardRegressor_\"+ experiment + \"/StandardRegressor_\"+ experiment + \"_checkpoints.ckpt\")\n",
    "\n",
    "\n",
    "train_config = dict(\n",
    "    # network architecture\n",
    "    criterion = nn.MSELoss,\n",
    "    hidden_dim = standard.hparams[\"hidden_dim\"],\n",
    "    dropout = standard.hparams[\"dropout\"],\n",
    "    # fairness parameters\n",
    "    sensitivity_parameter = 2.0,\n",
    "    fairness_constraints = [0.1, 0.1, 0.1],   # gamma for restricting DE, IE, SE\n",
    "    # training\n",
    "    batch_size = 32,\n",
    "    learning_rate = 0.0001,\n",
    "    max_epochs = 200,                                                                                                      \n",
    "    num_workers = 0,\n",
    "    nested_epochs_lagrangian = 3,    \n",
    "    # optimization\n",
    "    number_of_trials = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(5)\n",
    "\n",
    "experiment = \"prison\"\n",
    "\n",
    "data_location = \"../data/prison/prison_dataframe\"\n",
    "save_location = \"../models\"\n",
    "\n",
    "with open(data_location, \"rb\") as input:\n",
    "    full_data = pickle.load(input)\n",
    "full_data = full_data.drop([\"USE\", \"UIE\", \"UDE\"],axis=1)\n",
    "\n",
    "n_features = full_data.drop(\"Y\", axis = 1).shape[1]\n",
    "sensitive_attributes = full_data[\"A\"].unique()\n",
    "node_names = full_data.drop([\"Y\"],axis=1).columns.values\n",
    "\n",
    "train_data = full_data.iloc[:int(0.6*full_data.shape[0]),:]\n",
    "\n",
    "checkpointpath_density = save_location + \"/density_estimator_\" + experiment + \"/density_estimator_\" + experiment + \"_checkpoints.ckpt\"\n",
    "\n",
    "model_name = \"FairRegressor_\" + experiment + \"_sensitivity_\" + str(train_config[\"sensitivity_parameter\"])\n",
    "\n",
    "    \n",
    "final_model = FairRegressor(\n",
    "    n_features = n_features,\n",
    "    n_classes = 1,\n",
    "    train_data = train_data,\n",
    "    hidden_dim = train_config[\"hidden_dim\"],\n",
    "    dropout = train_config[\"dropout\"],\n",
    "    batch_size=train_config[\"batch_size\"],\n",
    "    criterion_pred = train_config['criterion'],\n",
    "    learning_rate = train_config[\"learning_rate\"],\n",
    "    sensitive_attributes = sensitive_attributes,\n",
    "    sensitivity_param= train_config[\"sensitivity_parameter\"],\n",
    "    checkpointpath_density=checkpointpath_density,\n",
    "    constraints = train_config['fairness_constraints'],\n",
    "    nested_epochs = train_config['nested_epochs_lagrangian'],\n",
    "    column_names = node_names\n",
    ")\n",
    "dm_final = FairClfDataModule(  \n",
    "    num_workers = train_config['num_workers'],\n",
    "    data_dir = data_location,\n",
    "    label_col = \"Y\",\n",
    "    batch_size = train_config[\"batch_size\"],\n",
    "    mode = \"prediction\"\n",
    ")\n",
    "logger = TensorBoardLogger(save_location, name = model_name)\n",
    "    \n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath= save_location + \"/\" + model_name,\n",
    "    filename= model_name + \"_checkpoints\",\n",
    "    save_top_k=5,\n",
    "    mode=\"min\",\n",
    ")\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=5, verbose=False, mode=\"min\", check_on_train_epoch_end=True)\n",
    "\n",
    "final_trainer = pl.Trainer(\n",
    "    max_epochs=train_config['max_epochs'],\n",
    "    logger = logger,\n",
    "    callbacks = [checkpoint_callback, early_stop_callback],\n",
    "    deterministic=True,\n",
    "    accelerator=\"cpu\"\n",
    ")\n",
    "\n",
    "final_trainer.fit(final_model, dm_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
